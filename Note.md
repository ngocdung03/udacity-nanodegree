##### Lesson 2: Binary classification models
- Logistic regression: 
    - Logit: most common
    - Probit
    - Log-log
    - * Remove variables that have too many categories within one predictor variable.
- Model comparision:
    - Accuracy, F1, AUC
    - Performance diagnostics plots:
        - Gain chart and Lift chart/curves: http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html
        - Precision and recall curve: https://www.quora.com/What-is-Precision-Recall-PR-curve
        - ROC curve: https://www.dataschool.io/roc-curves-and-auc-explained/ 
        - ...
    - *In the example: regression + stepwise perform the best
- Variable Status has some classes that are not statistically significant. Keep in mind:
    - It is not possible to keep only part of a categorical variable
    - If NO class in a categorical variable is significant, then you may consider not using the variable at all.
    - If ANY class in a categorical variable is significant, then you should keep it.
    - A combination of classes in a categorical variable is possible with testing, but it should make logical sense though.
        - In this case Status.Gold is significant but Status.Silver & Status.Platinum are not. To create a Gold vs. Other does not make logical sense since Gold is between Silver and Platinum.
- Forest model results:
    - *Out of the bag* error rate: 
        - How well the model performed
        - How solid the model performs with estimation data (similar to R-squared)
    - Confusion matrix: 
        - How well the model performed
        - Where error occured in classifying data (any trend or bias?)
    - The Percentage Error for Different Number of Trees graph: determine the ideal number of trees.
    - Variable Importance graph: focuson which variables are most associated with the data.
- Boosted model:
    - Instead of creating a bunch of random trees, the boosted model makes one tree.It analyzes on the errors of the tree to identify the biggest source of error. Then it changes the tree to reduce that error and the it does the analysis again to find the next biggest error...
    - Both accurate AND fast.
    - Mathematical foundation: https://en.wikipedia.org/wiki/Gradient_boosting
- Boosted model results:
    - Variable importance plot
    - Number of iterations assessment plot:     
        - shows that amount of variance, or deviance, that is captured with more iterations. 
        - How many trees are needed to create the optimal result?
- Project:
    - Cleanup, format, and blend a wide range of data sources
    - Build predictive classification models using Logistic Regression, Decision Tree, Random Forest, and Boosted Model

##### Lesson 3: A/B test
- Eperimental variables: outcome
- Control variables: to make sure that 2 groups are as similar as possible
- Determine which control variables should be used:
    - List potential variables
    - Is variable data available
    - Logical connection between control and target variable
    - Test the correlation between the control and target variables
    - Test correlation between other correlation variable
- Lurking variable (confounding): can cause you to overweight a control variable or include the control variables that aren't important
- Randomized design: 
    - little opportunity to control variables, volume and velocity of data is high enough that you are not worried about bias.
    - groups are not predetermined.
    - choose duration and size: ensure control and treatment groups are representative of the population -> considering control variables.
    - t-test
- Matched pair: 
    - when the volume of observations is fairly low, great concern for bias, high cost per observation.
    - must be set up ahead of time.
    - can yield significant results even with a low number of treatment units.
    - One cycle of experiment
    - Choose the treatment units:
        - Identify outliers
        - Decide number of treatment units
        - Randomly select
    - Treatment units >=10. Sometimes when the treatment units are low, the experiment may need to be repeated or run for a longer period of time.
    - Choose the control units:
        - Matched pair
        - Alteryx: AB Controls tool using KD tree: connect treatment data to T, control data to D
        - Balance the number of control units and average distance
    - Analysis: paired t-test
    - Data cleanup stage:
        - scrubbing out any outliers
        - getting rid of stores with missing data
        - matching control stores to treatment stores
            - Numeric measures are needed in order to match them.
            - Two of the best measures: trend and seasonality.
        - selecting the appropriate comparable period
    - To accommodate discrete variables, the units need to be matched with those that share a discrete characteristic.
    - Problem solving framework: You may have to try different combinations to find variable that give you the best matching results.
    - Select continuous control variables: 2 commons
        - Trend: Growth in store traffic
        - Seasonal patterns: in sales volume
    - AB test tool in Alteryx need data duration of 1 year + 6 periods.
    - Consider how long to run the experiment: if customer go to the store for a weekly basis -> at least 1 week 
    - Assuming a customer visits the spa about once every ten weeks: 
        - how long should our experiment last (how many weeks should we collect data)? 10 weeks.
        - Based on the duration of our experiment, how many weeks of data should we have for our comparison period? 10 weeks.
        - How many weeks of historical data should we have? 1 year + 12 weeks = 64 weeks
        - Join tool in Alteryx: L output - control candidate stores, J output - treatment stores.
        - A/B Analysis tool: P input - performance data,  I ouput - HTML based interaction dashboard, O output - textual summary.

##### Time series forecasting
- ARISMA, ETS models.
- Order matters: dependency on time.
- Sequential and equal interval measurement.
- One data point each time.
- Objectives:
    - Identify patterns.
    - Predict trends.
- Naive method: use data closest in time to predict.
    - If there is not enough data to create a predictive model, it can supplment forecasts for the near future.
- A trend cycle: uptrend, horizontal trend, downtrend.
- Seasonality: fixed period, associate with calendar. 
- Cyclical pattern: exists when data exhibits rises and falls not of a fixed perion often with the interval longer than seasonal pattern.
- Exponential Smoothing: ETS
    - Use weighted averages: more weight to the most recent observation.
    - ETS: Error, Trend, Seasonality. Each term can be applied either additively, multiplicatively, ...
    - [Time series decomposition plot.jpeg]
    - Additive model: when the trend and seasonal variation are relatively constant over time.
    - Multiplicative model: when they decrease or increase in magnitude over time.
    - [Trend for models.jpg], see more for plots of seasonal patterns in additive and multiplicative models.
    - If time series does not have a trend line and does not have seasonality component: *Simple Exponential Smoothing* model.
        - Forecast = Weight[t]*Y[t] + Weight[t-1]*Y[t-1] + Weight[t-2]*Y[t-2] + ... + α(1-α)^n Yn
        - The smoothing parameter α can be set for any value between 0 and 1.
        - Choosing the correct smoothing parameter is often an iterative process.
        - The simple exponential smoothing method does not account for any trend or seasonal components, rather, it only uses the decreasing weights to forecast future results. 
    - Double Exponential Smoothing (Holt's Linear Trend):
        - Additive
        - Include trend in calculation, not season
        - http://www.real-statistics.com/time-series-analysis/basic-time-series-forecasting/holt-linear-trend/
    - Exponential Trend Model: similar to Holt's Linear Trend with level and trend are applied multiplicatively.
    - Damped Trend Model: dampens the trend line into a flat line, some time into the future. Can apply in both additive and multiplicative calculation.
        - Can run a damped model and its original version side by side. Then check the forecasted errors to see which one fits time series better.
    - Holt-Winters Seasonal Method: all 3 components
        - Can be used with Damped parameters.
        - Most widely regarded methods for forecasting seasonal data.
        - Additive for trend.
        - Multiplicative and additive for seasonal components.
    - can generalize all of these models using a naming system for ETS: Error, Trend, Seasonality
        - for each component in the ETS system, we can assign None, Multiplicative, or Additive (or N, M, A) for each of the three components in our time series.
        - Examples: A time series model that has a constant error, linear trend, and increasing seasonal components means we would need to use an ETS model of: ETS(A,A,M)
    - Steps to building an ETS model: start off by determining how to apply the E, T, and S components based on your data. Then, you can create a holdout sample, build the model, and check the model’s forecast against the holdout sample.
- ARIMA: Auto, Regressive, Integrated, Moving, Average
    - Non seasonal ARIMA: represented by p, d, q
        - p (AR): periods to lag for, helps adjust the line fitted to forecast the series.
        - d (I): differencing, number of transformations used in the process, used to transform a time series into a stationary one.
        - q (MA): moving average, lag of the error component.
        - https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp
    - Stationary time series: mean and variance are constant over time.
        - Allow to obtain meaningful statistics: means, variances, and correlations with other variables.
        - The number of times of differencing needed to render the series stationary will be the differenced I(d) term in our ARIMA model.
        - *The best way to determine whether or not the series is sufficiently differenced is to plot the differenced series and check to see if there is a constant mean and variance.
    - Autocorrelation: how correlated a time series is with its past values.
        - Autocorrelation function plot
        - Suggest the time series is not stationary, need to be differenced
    - Generally use either AR or MA term. Models that use both are less common.
        - If the stationarized series has positive correlation at Lag-1, AR terms are best. (and spike decay to 0 in ACF plot)
        - If it has negative correlation at Lag-1, MA term are best.
        - [AR-MA-ACF-PACF.jpeg]
    - Partial correlation: the correlation between 2 variables controlling for the values of another set of variables.
        - https://online.stat.psu.edu/stat510/lesson/2/2.2
        - https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/#:~:text=A%20partial%20autocorrelation%20is%20a,relationships%20of%20intervening%20observations%20removed.
        - PACF: correlation of the points controlling for the values of all previous lag variables. Suggests how many AR terms needed to use to explain the article relation pattern in a time series.
            - If it drops off at lag-k: ARk Model
            - If it drops off more gradually: MA Model
    - *Autoregressive* component: The number of lags that correlated with current value. Determine how many AR in model by ACF and PACF -> suggest the best models to test against each others.
    - *Moving average* component: when the series undergoes random jumps whose effects are felt in two or more consecutive periods. 
        - The jumps are represented in the error calculated in the ARIMA model.
        - What the MA() component will lag for.
        - A purely MA() model will smooth out the impact of sudden movements in the data, similar to simple exponential smoothing methods. 
        - Negatively autocorrelated at Lag-1, ACF cuts off sharply after a few lags, PACF decreases out more gradually.
    - Remember you should always review the ACF and PACF of differenced series when making judgements on how many AR and MA terms to include.
    - Identifying the numbers of AR or MA terms in an ARIMA model: https://people.duke.edu/~rnau/411arim3.htm
    - *Integrated* Component: number of times we have to difference our dataset to make it stationary.
    - Seasonal ARIMA: ARIMA(p,d,q)(P,D,Q)m
         - m: periods in each season.
         - P,D,Q: terms for the seasonal part of the model.
         - Non-seasonal/seasonal differencing or combination
            - Adjust for seasonal difference:
            `Multi-Row Formula tool: Create New Field = Seasonal Difference; format Double; Num Rows = 12; Values that don't Exist = NULL; Expressions = [Bookings] - [Row-12:Bookings]`
            - Seasonal difference hasn't adjust for the effect of seasonality -> Adjust for first seasonal difference: 
            `Multi-row formula tool: Create New Field = S First Difference; Double; 1; NULL; Expressions = [Seasonal Difference] - [Row-1: Seasonal Difference]`
            - TS plot: plot S First Difference.
            - [ts-plot-tool-error-debugging.pdf]
        - Determine Seasonal AR, MA terms: account for seasonal lag.
            - When looking for non-seasonal p and q values in seasonally difference data, we can ignore any significance at the seasonal lags. 
            - For seasonal term P and Q: for monthly data, only consider lag 12, 24, and so on. If seasonal autocorrelation is (+): AR, (-): MA.
    - Set up ARIMA Tool: 
        - Model name = ARIMA, Target = [target], Monthly; 
        - Model customization tab = Completely user specified model -> set up value for pdq and PDQ.
        - Other options = Series starting period: specify year and start week/month...
    - Steps to build a ARIMA model: [Build your first ARIMA Model.docx]
- Important properties of the time series: Patterns of variation, Effects of seasonality, Removing autocorrelation.
- The assumptions on which you base your chosen model should agree with your intuition about how the time series is likely to behave in the future.
- Comparing criteria: Residual plots, forecasting errors, Akaike Information Criteria.
- Holdout sample: usually the most recent data points. The size depends: how long the time series is and how far you would like to forecast (ideally at least the periods you are forecasting for).
- Good time series forecasting model: uncorrelated residuals, ~0 mean of residuals.
    - After differencing or adding AR/MA terms, make sure almost the lags do not have a siginificant correlation in the ACF plot (residual plot). Otherwise, should add more term. 
    - If the residual is biased, add mean for all the forecasts.
- O node (output), I node (interactive) in TS Forecast tool
- Accuracy: what is being forecasted, what accuracy measure is used, what type of data set is used.
- Interpreting measures of error: [Interpreting measures of error.doc]
    - Scale dependents errors: such as mean error (ME) mean percentage error (MPE), mean absolute error (MAE) and root mean squared error (RMSE - lower means narrower range of possible values), are based on a set scale, and cannot be used to make comparisons that are on a different scale.
    - Percentage errors, like MAPE, are useful because they are scale independent, so they can be used to compare forecasts between different data series, unlike scale dependent errors. The disadvantage is that it cannot be used if the series has zero values.
    - Scale-free errors were introduced more recently to offer a scale-independent measure that doesn't have many of the problems of other errors like percentage errors. (MASE)
    - https://robjhyndman.com/papers/foresight.pdf
- Using AIC to test ARIMA models: https://coolstatsblog.com/2013/08/14/using-aic-to-test-arima-models-2/
    - Alteryx will automate the model selection process
- Also consider the confidence intervals when comparing the models.
- In simple words, PCA is a method of obtaining important variables (in form of components) from a large set of variables available in a data set. It extracts low dimensional set of features by taking a projection of irrelevant dimensions from a high dimensional data set with a motive to capture as much information as possible. With fewer variables obtained while minimising the loss of information, visualization also becomes much more meaningful. PCA is more useful when dealing with 3 or higher dimensional data.

##### Segmentation and Clustering
- Standardization: deal with data as a whole, treat all objects in the same way.
- Localization: customize based on specifics of the individual.
    + Good: meet local needs, optimize results of the store.
    + Bad: Huge amount of extra costs, more resources needed to accomplish it.
- Clustering/segmentation
- Data type in clustering:
    - If categorical variable related to numerical: assign number as ordinal variable
- Data quality: missing and outliers
- Scaling: z-score, or subtract difference/max
- Transforming variables:
    - Date variables: calculate mean or median of the fields, use that date as a reference point to create a before and after value.
- Variable reduction:
    - Use when variables are related
    - PCA: Accounts for total variation. Useful when trying to explain as much of total variance as possible or the total picture of the data -> may not easy to interpret.
    - Factor analysis: Correlation between variables. Useful when trying to understand factor(s) from a common set of variables or explain the correlations between the variables. (Eg: Social science, attitudinal study)
    - Loadings: effect that each particular variable has on that component, size matters but sign doesn't matter.
    - Rule of thumb: capture 80% of total variance.
    - In course example, variables related to snow are not scaled because all the values for snow fall in July are zero. The scaling process doesn't work and would thrown an error.
- Clustering Techniques:
    - Hierarchical: Finding the closest distance between entities. No need to predetermine how many clusters. Slow with a large set of objects.
        - Agglomerative/bottom up: each observation starts as one cluster and merge up (single/complete/average/centroid linkage).
        - Divisive/top down: all of the objects are in a single cluster, remove the outsiders from the least cohesive cluster.
    - K-centroid: Finding the objects closest to the centroids. Specify k number of centroids. Works well when clusters are equal sizes in densities. Highly sensitive with outliers -> should scale the fields.
        - The locations of the seed centroids can be critical.
        - Better solution: multiple starting random seeds and averaged results.
        - Choosing k: 
            - Based on study objective
            - Small # clusters don't fit all descriptors
            - Large # may not work as well operationally.
    - Tracking # clusters:
        - For Hierarchical: using Dendogram.
        - For k-centroid: internal/external cluster validation - test a cluster solution to see how 'good' it is. 
            - Adjusted rand index: how similar the objects within a cluster are (cluster stability)
            - Calinski-Harabasz index(CH): both compactness and distinctness of the clusters.
            - Find the # in the box-plot report of Alteryx so that median is high and the spread is minimized.
            - When comparing methods, note the scale of the indices
            - Reconsiderint the variables used.
    - Interpreting cluster results:
        - Size: if a cluster is abnormally small in relative size -> unusual or outliers, look specifically why this is the case.
        - Avg ditance within the cluster: the smaller the more compact
        - Max distance: distance from the point farthest from the centroid, show how far the farthest outlier.
        - Separation: distant of the closest point not in the relevant cluster, the larger the better
        - Characterize each cluster by variable: 
            - look only into within variable comparisons - a high positive value versus a high negative indicate for that variable, those 2 clusters are opposite.
            - the values don't indicate real ordinal values. (They don't necessarily have the most absolute value but the most different)
    - In Alteryx, export the list with only Store ID and Cluster ID
    - It is best validate your clusters using variables that were not directly included in the cluster modeling process
    - Communicating the 'story' and ongoing testing: 
        - Brief names of the clusters that summarize the findings.
        - When apply and translate the result of cluster analysis, may test with a limited number of objects first: eg. A/B testing
        - Cluster analysis should remain an iterative process.
##### Tableau
- Top 10 Tableau table calculations: https://www.tableau.com/about/blog/top-10-tableau-table-calculations
- Split a field into multiple fields: https://help.tableau.com/current/pro/desktop/en-us/split.htm#troubleshooting-splits-and-custom-splits
- Combining data: https://help.tableau.com/current/pro/desktop/en-us/joining_tables.htm
- Hierarchies: Tableau automatically creates hỉerarchy for date variable (year>quarter>month) but you can create manual hierarchies.
    - If data by date looks discrete, can convert it into continuous.
- Filter in Tableau: https://www.tableau.com/learn/tutorials/on-demand/using-filter-shelf
- Priority for encoding: postion, length > color, shape > size.
- Small multiples and dual axis: for dual axis, can drag 2nd measure to the right side of the plot.
    - https://evolytics.com/blog/tableau-201-how-to-make-small-multiples/
- Map configuration: https://help.tableau.com/current/pro/desktop/en-us/maps_editlocation.htm
- Is dual-scaled axes good?: https://www.perceptualedge.com/articles/visual_business_intelligence/dual-scaled_axes.pdf
- Groups and sets: Sets are dynamic (categorize data by conditions - >,<...) while groups are static
- Calculated Fields: with > 2 categories, group may not be a best idea.
    - Eg: IF SUM([Sales]) > 10000 THEN "Good" ELSE "Bad"
    - IIF(SUM([Sales]) > 10000, "Good", "Bad")
    - Calculations with strings: https://www.tableau.com/learn/tutorials/on-demand/string-calculations ; https://www.clearlyandsimply.com/clearly_and_simply/2014/06/string-calculations-in-tableau.html
    - More: https://help.tableau.com/current/pro/desktop/en-us/calculations_calculatedfields.htm
- Table calculations: useful for helping you to compare the data that exists in a plot to other parts of the plot.
- Other resources: 
    - Free training videos: https://www.tableau.com/learn/training/20213
    - Creating an overlapped bar chart: https://kb.tableau.com/articles/howto/dual-axis-bar-chart-multiple-measures

##### Capstone project
<<<<<<< HEAD
- Finish capstone project
=======
- Begin task 3
>>>>>>> 0ba2af14d7e18ccfa9d881e8fdafe9d7b0f146b8
