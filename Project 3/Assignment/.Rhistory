y_true <-  c(30, 30, 20, 20)
event <-  c(1, 0, 1, 0)
scores <-  c(10, 5, 15, 20)
harell_c(y_true,scores,event)
harrell_c(y_true,scores,event)
# Function for c-index:
harrell_c <- function(y_true, scores, event){
# '''
# Compute Harrel C-index given true event/censoring times,
# model output, and event indicators.
# Args:
# y_true (array): array of true event times
# scores (array): model risk scores
# event (array): indicator, 1 if event occurred at that index, 0 for␣
# ,→censorship
# Returns:
# result (float): C-index metric
# '''
n = length(y_true)
#assert (len(scores) == n and len(event) == n)
concordant = 0.0
permissible = 0.0
ties = 0.0
result = 0.0
### START CODE HERE (REPLACE INSTANCES OF 'None' and 'pass' with your code)␣→###
# use double for loop to go through cases
for (i in 1:n){
# set lower bound on j to avoid double counting
for(j in (i+1): n){
# check if at most one is censored
if(event[i]==1 | event[j]==1){
# check if neither are censored
if(event[i]==1 & event[j]==1){
permissible <- permissible+1
# check if scores are tied
if(scores[i]==scores[j]){
ties = ties + 1}
# check for concordant
else if(scores[i]>scores[j] & y_true[i]<y_true[j]){
concordant = concordant + 1}
else if(scores[i]<scores[j] & y_true[j]<y_true[i]){
concordant = concordant + 1}
# check if one is censored
}
else if(event[i]==0 | event[j]==0){
# get censored index
censored = j
uncensored = i
if(event[i] == 0){
censored = i
uncensored = j}
# check if permissible
# Note: in this case, we are assuming that censored at a time
# means that you did NOT die at that time. That is, if you
# live until time 30 and have event = 0, then you lived THROUGH
# time 30.
if(y_true[uncensored] <= y_true[censored]){
permissible <- permissible+1
# check if scores are tied
if(scores[uncensored]==scores[censored]){
# update ties
ties = ties + 1}
# check if scores are concordant
if(scores[uncensored]>scores[censored]){
concordant = concordant + 1}
}}
# set result to c-index computed from number of concordant pairs,
# number of ties, and number of permissible pairs (REPLACE 0 with your␣→code
}}}
result = (concordant + (0.5 * ties)) / permissible
return(result)
}
harrell_c(y_true,scores,event)
library(randomForestSRC)
data <- read.csv('C:/Users/ngocdung/Dropbox/MCC/Admin/Entry and scoring.csv', stringAsFactor = F)
rm(list = ls())
library(tidyverse)
data <- read.csv('C:/Users/ngocdung/Dropbox/MCC/Admin/Entry and scoring.csv', stringAsFactor = F)
data <- read.csv('C:/Users/ngocdung/Dropbox/MCC/Admin/Entry and scoring.csv', stringsAsFactor = F)
library(ggplot2)
data %>%
#  filter(country == "United States") %>%
ggplot(aes(stylist, avg_score)) +
geom_bar()
glimpse(data)
data %>%
#  filter(country == "United States") %>%
ggplot(aes(stylist)) +
geom_bar(stat="avg_score")
data %>%
#  filter(country == "United States") %>%
ggplot() +
geom_bar(stat="avg_score",
position = "stylist",)
data %>%
#  filter(country == "United States") %>%
ggplot(avg_score, aes(stylist)) +
geom_bar()
data %>%
#  filter(country == "United States") %>%
ggplot(aes(stylist)) +
geom_bar()
data %>%
#  filter(country == "United States") %>%
ggplot(aes(stylist, avg_score)) +
geom_bar()
data %>%
#  filter(country == "United States") %>%
ggplot(aes(stylist, avg_score)) +
geom_col()
data %>%
#  filter(country == "United States") %>%
ggplot(aes(stylist, na.omit(avg_score))) +
geom_col()
View(data)
View(data)
data %>%
filter(!is.na(avg_score)) %>%
ggplot(aes(stylist, avg_score)) +
geom_col()
data %>%
filter(!is.na(avg_score)) %>%
group_by(stylist) %>%
summary(mean_score=mean(avg_score))
data %>%
filter(!is.na(avg_score)) %>%
group_by(stylist) %>%
summarize(mean_score=mean(avg_score))
data %>%
filter(!is.na(avg_score)) %>%
group_by(stylist) %>%
summarize(average_score=mean(avg_score))
data %>%
data %>%
data %>%
filter(!is.na(avg_score)) %>%
group_by(stylist) %>%
summarize(average_score=mean(avg_score)) %>%
ggplot(aes(stylist, avg_score = average_score)) +
geom_col()
data %>%
filter(!is.na(avg_score)) %>%
group_by(stylist) %>%
summarize(average_score=mean(avg_score)) %>%
ggplot(aes(stylist, average_score)) +
geom_col()
data %>%
filter(!is.na(avg_score)) %>%
group_by(stylist) %>%
summarize(average_score=mean(avg_score))
data %>%
filter(!is.na(avg_score)) %>%
group_by(slot) %>%
summarize(average_score=mean(avg_score)) %>%
ggplot(aes(slot, average_score)) +
geom_col()
data %>%
filter(!is.na(avg_score)) %>%
group_by(slot) %>%
summarize(average_score=mean(avg_score)) %>%
ggplot(aes(as.factor(slot), average_score)) +
geom_col()
# Slot - entry
data %>%
filter(!is.na(total_entry)) %>%
# group_by(slot) %>%
# summarize(=mean(total_entry)) %>%
ggplot(aes(as.factor(slot), total_entry)) +
geom_col()
# Slot - score
data %>%
filter(!is.na(avg_score)) %>%
group_by(slot) %>%
summarize(average_score=mean(avg_score)) %>%
ggplot(aes(as.factor(slot), average_score)) +
geom_col() +
ggtitle('Average score by Slot')
# Slot - entry
data %>%
filter(!is.na(total_entry)) %>%
group_by(slot) %>%
summarize(avg_entry=mean(total_entry)) %>%
ggplot(aes(as.factor(slot), avg_entry)) +
geom_col() +
ggtitle('Average number of entries by Slot')
data %>% geom_bar(sum(vlcv), sum(lcv), sum(mcv))
geom_bar(sum(data$vlcv), sum(data$lcv), sum(data$mcv))
geom_bar(aes(sum(data$vlcv), sum(data$lcv), sum(data$mcv)))
sum(data$vlcv)
geom_bar(aes(sum(data$vlcv_no), sum(data$lcv_no), sum(data$mcv_no)))
sum(data$vlcv_no)
glimpse(data)
# Entry by stylist
data %>%
filter(!is.na(total_entry)) %>%
group_by(stylist) %>%
summarize(avg_entry=mean(total_entry)) %>%
ggplot(aes(stylist, avg_entry)) +
geom_col() +
ggtitle('Average number of entries by Stylist')
.libPaths()
?install.packages
install.packages('randomForestSRC', dep = TRUE)
loadNamespace('randomSurvivalSRC')
loadNamespace('randomForestSRC')
R.home()
.libPaths()
sd(c(58, 89,65, 78, 55,26,93,46,43,59))
1.96*11/6
3.59+22
2/sqrt(3)
-.48/sqrt(3)
sqrt(3)
a = c(65    ,	78,    	52,    	82,    	92,    	89,    	73,    	98,    	56,    	76 )
b= c( 39,   	43,	21,	64,	57,	47,	27,	75,	34,	52,
)
b= c( 39,   	43,	21,	64,	57,	47,	27,	75,	34,	52)
lm(a, b)
lm(b~a)
lm(a~b)
122/11
95+27*3
176/11
18*11-81
117/33
117/3
39*3
117-18
18*11-81
39*3-18
7-18
77+18
18*11-81
117/33
66-27
117/3
39*3-18
77+18
6*3
1.25*20000
sqrt(8^2-4^2)
1000/2.5
400*0.5
4/3
sqrt(3-5*x+x^2+x^3)/(x-1)
x = 1.1
sqrt(3-5*x+x^2+x^3)/(x-1)
x = 1.01
sqrt(3-5*x+x^2+x^3)/(x-1)
x = 1.001
sqrt(3-5*x+x^2+x^3)/(x-1)
a = c(106, 110, 108, 112, 128, 96, 103, 110, 117, 109)
sd(a)
x = c( 1,	2,	3,	4,	5,	6,	7,	8)
y = c(590,   	750,   	1215,  	1335,  	1830,  	1920,  	2265,  	2670
)
glm(x,y)
lm(y~x)
lm(x~)
lm(x~y)
## ------------------------------------------------------------
## White wine classification example
## ------------------------------------------------------------
## load the data
data(wine, package = "randomForestSRC")
wine$quality <- factor(wine$quality)
## default tuning call
o <- tune(quality ~ ., wine, doBest = TRUE)
library(randomForestSRC)
wine$quality <- factor(wine$quality)
## default tuning call
o <- tune(quality ~ ., wine, doBest = TRUE)
## here is the optimized forest
print(o$rf)
## visualize the nodesize/mtry OOB surface
if (library("akima", logical.return = TRUE)) {
## nice little wrapper for plotting results
plot.tune <- function(o, linear = TRUE) {
x <- o$results[,1]
y <- o$results[,2]
z <- o$results[,3]
so <- interp(x=x, y=y, z=z, linear = linear)
idx <- which.min(z)
x0 <- x[idx]
y0 <- y[idx]
filled.contour(x = so$x,
y = so$y,
z = so$z,
xlim = range(so$x, finite = TRUE) + c(-2, 2),
ylim = range(so$y, finite = TRUE) + c(-2, 2),
color.palette =
colorRampPalette(c("yellow", "red")),
xlab = "nodesize",
ylab = "mtry",
main = "error rate for nodesize and mtry",
key.title = title(main = "OOB error", cex.main = 1),
plot.axes = {axis(1);axis(2);points(x0,y0,pch="x",cex=1,font=2);
points(x,y,pch=16,cex=.25)})
}
## plot the surface
plot.tune(o)
}
# Random forest ####
library(randomForestSRC)
?tune.rfsrc
?rfsrc
a <- c()
a
a.append(1)
rm(list = ls())
#memory.limit(size=20000)
library(tidyverse)
library(lubridate)
library(caret)
library(randomForest)
#library(Hmisc)
#library(rms)
setwd("C:/Users/ngocdung/Dropbox/Front-end developer/src/data-science-path/udacity-nanodegree/Project 3/Assignment")
raw <- read.csv("./Creditworthiness/credit-data-training.csv", stringsAsFactors=F)
validation <- read.csv("./Creditworthiness/customers-to-score.csv", stringsAsFactors=F)
colnames(raw) <- str_replace_all(colnames(raw), '\\.', '_')
colnames(validation) <- str_replace_all(colnames(validation), '\\.', '_')
glimpse(raw)
glimpse(validation)
## 1. Building the dataing set
# 1.1. Identifying and imputing missing
isna = vector(length = length(raw))
i=1
for(col in raw){
isna[i] = sum(is.na(col))
i = i + 1 }
print(data.frame(colnames(raw), isna))  #Duration address - 344, Age years - 12
# Impute Age_years
median_age <- median(raw$Age_years[!is.na(raw$Age_years)])
raw$Age_years[is.na(raw$Age_years)] <- median_age
#View(raw[is.na(raw$Age_years),c("Credit_Application_Result","Age_years")])
# Drop Duration_in_Current_address
raw = raw[,!(colnames(raw) %in% c('Duration_in_Current_address'))]
# 1.2. Identifying low-variability fields and removing them
sapply(raw, unique)
num_cols <-  c('Duration_of_Credit_Month', 'Credit_Amount', 'Age_years') #exclude outcome and potential factors
factor <-  c('Instalment_per_cent',
#'Duration_in_Current_address',
'Most_valuable_available_asset',
'Type_of_apartment',
'Occupation',
'No_of_dependents',
'Telephone',
'Foreign_Worker')
remain <- colnames(raw[!colnames(raw) %in% c(num_cols, factor)])
count(raw, Instalment_per_cent)
count(raw, Most_valuable_available_asset)
count(raw, Type_of_apartment)
count(raw, Occupation)  #1-500
count(raw, No_of_dependents)  #1-427, 2-73
count(raw, Telephone)
count(raw, Foreign_Worker)  #1-481, 2-19
count(raw, Credit_Application_Result)
count(raw, Account_Balance)
count(raw, Payment_Status_of_Previous_Credit)
count(raw, Purpose)  #Draw histo - temporarily accepted
count(raw, Value_Savings_Stocks)
count(raw, Length_of_current_employment)
count(raw, Guarantors)  #1-457, 2-43
count(raw, Concurrent_Credits)  #1-500
count(raw, No_of_Credits_at_this_Bank)
# data_scale <- scale(raw[c(num_cols, factor)], center = T, scale = T)
# apply(data_scale,2, var)
# 1.3. Identifying highly-correlate fields (at least 0.7)
factor1 <- c('Instalment_per_cent',    #? Can treat as ordered numeric
'Most_valuable_available_asset',
#'Telephone',
'Type_of_apartment')
remain1 <- c("Credit_Application_Result",
"Account_Balance",
"Payment_Status_of_Previous_Credit",
"Purpose",
"Value_Savings_Stocks",
"Length_of_current_employment",
"No_of_Credits_at_this_Bank")
raw <- raw %>% mutate(Credit_Application_Result= as.factor(Credit_Application_Result),
Account_Balance= as.factor(Account_Balance),
Payment_Status_of_Previous_Credit=as.factor(Payment_Status_of_Previous_Credit),
Purpose=as.factor(Purpose),
Value_Savings_Stocks=as.factor(Value_Savings_Stocks),
Length_of_current_employment=as.factor(Length_of_current_employment),
No_of_Credits_at_this_Bank=as.factor(No_of_Credits_at_this_Bank))
a <- cor(raw[c(num_cols,factor1,remain1)])
# a[a==1] <- 0
# max(a) #0.5739797
cor(raw[c(num_cols)])
data <- raw[c(num_cols,factor1,remain1)]
colnames(data)  #13
## 2. Train your Classification Models
#Bias/Fairness: Are Opportunity/Odds/Accuracy same across different groups ?
#Prediction bias = average of predictions - average of lables in dataset
#Is under predict = bias?
set.seed(1)
train.index <- createDataPartition(data$Credit_Application_Result, p = .7, list = FALSE)
train <- data[train.index,]  #351
test <- data[-train.index,]  #149
# Logistic regression
model1 <- train(Credit_Application_Result~., data=train, method='glm')  #check
y_hat1 <- predict(model1, test)
y_hat10 <- predict(model1, train)
confusionMatrix(data = y_hat1, reference = test$Credit_Application_Result)            #$overall["Accuracy"]
confusionMatrix(data = y_hat10, reference = train$Credit_Application_Result) #Bias
vi1 <- varImp(model1, scale = F)   #pvalue
ggplot(a, top = dim(a$importance)[1]) +
ggtitle('Variable Importance - Logistic Regression')
?train
library(rf)
library(ada)
install.packages('ada')
library(ada)
model2 <- train(Credit_Application_Result~., data=train, method='rpart')
y_hat2 <- predict(model2, test)
y_hat20 <- predict(model2, train)
confusionMatrix(data = y_hat2, reference = test$Credit_Application_Result)
confusionMatrix(data = y_hat20, reference = train$Credit_Application_Result)
model3 <- train(Credit_Application_Result~., data=train, method='rf')
y_hat3 <- predict(model3, test)
y_hat30 <- predict(model3, train)
confusionMatrix(data = y_hat3, reference = test$Credit_Application_Result)
confusionMatrix(data = y_hat30, reference = train$Credit_Application_Result)
confusionMatrix(data = y_hat1, reference = test$Credit_Application_Result)            #$overall["Accuracy"]
model4 <- train(Credit_Application_Result~., data=train, method='ada')  #Boosted Classification Trees
y_hat4 <- predict(model4, test)
y_hat40 <- predict(model4, train)
confusionMatrix(data = y_hat4, reference = test$Credit_Application_Result)
confusionMatrix(data = y_hat40, reference = train$Credit_Application_Result)
library(plyr)
library(dplyr)
# Boosted model
model4 <- train(Credit_Application_Result~., data=train, method='ada')  #Boosted Classification Trees
?train.ada
?ada
# Boosted model
model4 <- train(Credit_Application_Result~., data=train, method='ada',
iter=20,nu=1,type="discrete")  #Boosted Classification Trees
install.packages('bst')
library(bst)
# Boosted model
model4 <- train(Credit_Application_Result~., data=train, method='bstTree')  #Boosted Classification Trees
confusionMatrix(data = y_hat3, reference = test$Credit_Application_Result)
?bst
# Boosted model
model4 <- train(Credit_Application_Result~., data=train, method='bstTree', learner=c('tree'))  #Boosted Classification Trees
?ada
colnames(train[-c('Credit_Application_Result')])
colnames(train[-1])
# Boosted model
model4 <- train(train[-1],
train$Credit_Application_Result,
method='bstTree',
learner='tree')  #Boosted Classification Trees
warnings()
# Boosted model
model4 <- bst(train[-1],
train$Credit_Application_Result,
learner='tree')  #Boosted Classification Trees
# Boosted model
model4 <- bst(train[-1],
train['Credit_Application_Result'],
learner='tree')  #Boosted Classification Trees
head(train['Credit_Application_Result'])
as.factor(train['Credit_Application_Result'])
View(train['Credit_Application_Result'])
factor(train['Credit_Application_Result'])
head(factor(train['Credit_Application_Result']))
as.factor(train$Credit_Application_Result)
class(train$Credit_Application_Result)
as.numeric(train$Credit_Application_Result)
# Boosted model
model4 <- bst(train[-1],
as.numeric(train['Credit_Application_Result']),
learner='tree')  #Boosted Classification Trees
# Boosted model
model4 <- bst(train[-1],
as.numeric(train$Credit_Application_Result),
learner='tree')  #Boosted Classification Trees
y_hat4 <- predict(model4, test)
y_hat40 <- predict(model4, train)
confusionMatrix(data = y_hat4, reference = as.numeric(test$Credit_Application_Result))
head(y_hat4)
unique(y_hat4)
head(y_hat3)
?ifelse
y_hat4c <- fifelse(y_hat4==1.001468, 1, 2)
y_hat4c <- ifelse(y_hat4==1.001468, 1, 2)
head(y_hat4c)
View(as.data.frame(y_hat4, y_hat4c))
head(as.numeric(train$Credit_Application_Result))
y_hat4c <- ifelse(y_hat4==1.001468, 2, 1)
head(y_hat4c)
y_hat4
y_hat4c
y_hat4c <- ifelse(y_hat4<1.5, 1, 2)
y_hat4c
confusionMatrix(data = y_hat4c, reference = as.numeric(test$Credit_Application_Result))
head(as.numeric(test$Credit_Application_Result))
unique(as.numeric(test$Credit_Application_Result))
unique(y_hat4c)
class(y_hat4c)
y_hat4c <- as.factor(ifelse(y_hat4<1.5, 1, 2))
confusionMatrix(data = y_hat4c, reference = as.factor(as.numeric(test$Credit_Application_Result)))
y_hat40 <- predict(model4, train)
y_hat40c <- as.factor(ifelse(y_hat40<1.5, 1, 2))
confusionMatrix(data = y_hat40c, reference = as.factor(as.numeric(train$Credit_Application_Result)))
confusionMatrix(data = y_hat4c, reference = as.factor(as.numeric(test$Credit_Application_Result)))
vi4 <- varImp(model4, scale = F)   #pvalue
?bst
