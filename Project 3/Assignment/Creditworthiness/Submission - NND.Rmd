---
title: 'Project Creditworthiness'
author: "Dung Nguyen"
output: 
  pdf_document:
    latex_engine: lualatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Step 1: Business and Data Understanding
●	What decisions needs to be made?
Determining whether customers are creditworthy to give a loan to.

●	What data is needed to inform those decisions?
Outcome: Credit-Application-Result.

●	What kind of model (Continuous, Binary, Non-Binary, Time-Series) do we need to use to help make these decisions?
Binary model.

## Step 2: Building the Training Set
After importing, the data set were formatted as suggested.
```{r setwd, echo=FALSE}
setwd("C:/Users/ngocdung/Dropbox/Front-end developer/src/data-science-path/udacity-nanodegree/Project 3/Assignment")
raw <- read.csv("./Creditworthiness/credit-data-training.csv", stringsAsFactors=F)
validation <- read.csv("./Creditworthiness/customers-to-score.csv", stringsAsFactors=F)
```

```{r import, message=c(32), warning=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(bst)
library(cowplot)
#raw <- read.csv("./Creditworthiness/credit-data-training.csv", stringsAsFactors=F)
#validation <- read.csv("./Creditworthiness/customers-to-score.csv", stringsAsFactors=F)
colnames(validation) <- str_replace_all(colnames(validation), '\\.', '_')
colnames(raw) <- str_replace_all(colnames(raw), '\\.', '_')
glimpse(raw)

```

### Identifying and imputing missing data
The number of missing values in each field:
```{r missing}
isna = vector(length = length(raw))
i=1
for(col in raw){
  isna[i] = sum(is.na(col))
  i = i + 1 }
print(data.frame(field = colnames(raw), no_of_missing = isna))
```

`Duration_in_Current_address` has 344 missing values so I dropped it.
`Age_years` has 12 missing values so I imputed it by median.
```{r impute and drop}
raw = raw[,!(colnames(raw) %in% c('Duration_in_Current_address'))]

median_age <- median(raw$Age_years[!is.na(raw$Age_years)])
raw$Age_years[is.na(raw$Age_years)] <- median_age
mean(raw$Age_years)
```

### Identifying low-variability fields and removing them
Frequency table and histogram of low-variability fields:
```{r drop}
print(list(count(raw, Occupation), 
     count(raw, No_of_dependents),
     count(raw, Foreign_Worker),
     count(raw, Guarantors),
     count(raw, Concurrent_Credits)))  

p1 <- histogram(raw$Occupation)
p2 <- histogram(raw$No_of_dependents)
p3 <- histogram(raw$Foreign_Worker)
p4 <- histogram(as.factor(raw$Guarantors))
p5 <- histogram(as.factor(raw$Concurrent_Credits))
plot_grid(p1, p2, p3, p4, p5)
```

After removing those fields plus `Telephone`, no high correlation was detected among numeric fields:
```{r correlation}
numeric <-  c('Duration_of_Credit_Month', 
              'Credit_Amount', 
              'Age_years')
numeric2 <- c('Instalment_per_cent',   
           'Most_valuable_available_asset',
           'Type_of_apartment')
factor <- c("Credit_Application_Result",
             "Account_Balance",
             "Payment_Status_of_Previous_Credit",
             "Purpose",
             "Value_Savings_Stocks",
             "Length_of_current_employment",
             "No_of_Credits_at_this_Bank")

raw <- data.frame(raw[c(numeric, numeric2)],
                     apply(raw[factor],2, as.factor))

print(cor(raw[c(numeric, numeric2)]))
```

The final data set has 13 columns
```{r final}
data <- raw
glimpse(data) 
```


## Step 3: Train your Classification Models
### Create Estimation and Validation samples
```{r train test set}
data$worthy <- as.factor(ifelse(data$Credit_Application_Result == 'Creditworthy', 1, 0))
data$nonworthy <- as.factor(ifelse(data$Credit_Application_Result == 'Non-Creditworthy', 1, 0))

set.seed(1)
train.index <- createDataPartition(data$Credit_Application_Result, p = .7, list = FALSE)  
train <- data[train.index,]
test <- data[-train.index,] 
```

### Confusion matrices
```{r training}
# Logistic regression
lr <- vector()
model1 <- train(Credit_Application_Result~., 
                data=train[!colnames(train) %in% c('worthy', 'nonworthy')], 
                method='glm')  #check
y_hat1 <- predict(model1, test)
y_hat10 <- predict(model1, train)
lr['Accuracy_train'] <- confusionMatrix(data = y_hat10, reference = train$Credit_Application_Result)$overall["Accuracy"] #Bias
lr['Accuracy_test'] <- confusionMatrix(data = y_hat1, reference = test$Credit_Application_Result)$overall["Accuracy"]

model1w <- train(worthy~., 
                  data=train[!colnames(train) %in% c('Credit_Application_Result', 'nonworthy')], 
                  method='glm')
y_hat1w <- predict(model1w, test)
lr['Accyracy_worthy'] <- confusionMatrix(data = y_hat1w, reference = test$worthy)$overall["Accuracy"]

model1n <- train(nonworthy~., 
                  data=train[!colnames(train) %in% c('Credit_Application_Result', 'worthy')], 
                  method='glm')
y_hat1n <- predict(model1n, test)
lr['Accuracy_nonworthy'] <- confusionMatrix(data = y_hat1n, reference = test$nonworthy)$overall["Accuracy"]

vi1 <- varImp(model1, scale = F)   #pvalue
plot1 <- ggplot(vi1, top = dim(vi1$importance)[1]) + 
  ggtitle('Variable Importance - Logistic Regression')

# Decision tree
tree <- vector()
model2 <- train(Credit_Application_Result~., 
                data=train[!colnames(train) %in% c('worthy', 'nonworthy')],
                method='rpart')
y_hat2 <- predict(model2, test)
y_hat20 <- predict(model2, train)
tree['Accuracy_train'] <- confusionMatrix(data = y_hat20, reference = train$Credit_Application_Result)$overall["Accuracy"] 
tree['Accuracy_test'] <- confusionMatrix(data = y_hat2, reference = test$Credit_Application_Result)$overall["Accuracy"]            

model2w <- train(worthy~., 
                 data=train[!colnames(train) %in% c('Credit_Application_Result', 'nonworthy')], 
                 method='rpart')
y_hat2w <- predict(model2w, test)
tree['Accuracy_worthy'] <- confusionMatrix(data = y_hat2w, reference = test$worthy)$overall["Accuracy"]

model2n <- train(nonworthy~., 
                 data=train[!colnames(train) %in% c('Credit_Application_Result', 'worthy')], 
                 method='rpart')
y_hat2n <- predict(model2n, test)
tree['Accuracy_nonworthy'] <- confusionMatrix(data = y_hat2n, reference = test$nonworthy)$overall["Accuracy"]

vi2 <- varImp(model2, scale = F)   
plot2 <- ggplot(vi2, top = dim(vi2$importance)[1]) + 
  ggtitle('Variable Importance - Decision Tree')

# Forest model
forest <- vector()
set.seed(1)
model3 <- train(Credit_Application_Result~., 
                data=train[!colnames(train) %in% c('worthy', 'nonworthy')], 
                method='rf',
                ntree=500)
y_hat3 <- predict(model3, test)
y_hat30 <- predict(model3, train)
forest['Accuracy_train'] <- confusionMatrix(data = y_hat30, reference = train$Credit_Application_Result)$overall["Accuracy"]
forest['Accuracy_test'] <- confusionMatrix(data = y_hat3, reference = test$Credit_Application_Result)$overall["Accuracy"]   

set.seed(1)
model3w <- train(worthy~., 
                 data=train[!colnames(train) %in% c('Credit_Application_Result', 'nonworthy')], 
                 method='rf',
                 ntree=500)
y_hat3w <- predict(model3w, test)
forest['Accuracy_worthy'] <- confusionMatrix(data = y_hat3w, reference = test$worthy)$overall["Accuracy"]

set.seed(1)
model3n <- train(nonworthy~., 
                 data=train[!colnames(train) %in% c('Credit_Application_Result', 'worthy')], 
                 method='rf',
                 ntree=500)
y_hat3n <- predict(model3n, test)
forest['Accuracy_nonworthy'] <- confusionMatrix(data = y_hat3n, reference = test$nonworthy)$overall["Accuracy"]

vi3 <- varImp(model3, scale = F)   #pvalue
plot3 <- ggplot(vi3, top = dim(vi3$importance)[1]) + 
  ggtitle('Variable Importance - Random Forest')
  
# Boosted model - Boosted Classification Trees
boosted <- vector()
# Data have to be re-formatted for bst() function
train4 <- train %>% mutate(#Credit_Application_Result= as.numeric(Credit_Application_Result),
  Account_Balance= as.factor(as.numeric(Account_Balance)),
  Payment_Status_of_Previous_Credit=as.factor(as.numeric(Payment_Status_of_Previous_Credit)),
  Purpose=as.factor(as.numeric(Purpose)),
  Value_Savings_Stocks=as.factor(as.numeric(Value_Savings_Stocks)),
  Length_of_current_employment=as.factor(as.numeric(Length_of_current_employment)),
  No_of_Credits_at_this_Bank=as.factor(as.numeric(No_of_Credits_at_this_Bank)),
  )

test4 <- test %>% mutate(#Credit_Application_Result= as.numeric(Credit_Application_Result),
  Account_Balance= as.factor(as.numeric(Account_Balance)),
  Payment_Status_of_Previous_Credit=as.factor(as.numeric(Payment_Status_of_Previous_Credit)),
  Purpose=as.factor(as.numeric(Purpose)),
  Value_Savings_Stocks=as.factor(as.numeric(Value_Savings_Stocks)),
  Length_of_current_employment=as.factor(as.numeric(Length_of_current_employment)),
  No_of_Credits_at_this_Bank=as.factor(as.numeric(No_of_Credits_at_this_Bank)),
  )
control <- trainControl(method = "cv", number = 5)
set.seed(1)
model4 <- train(Credit_Application_Result~., 
                data=train4[!colnames(train) %in% c('worthy', 'nonworthy')], 
                method="bstTree", 
                trControl = control)
y_hat4 <- predict(model4, test4)
y_hat40 <- predict(model4, train4)
boosted['Accuracy_train'] <- confusionMatrix(data = y_hat40, reference = train4$Credit_Application_Result)$overall["Accuracy"]  
boosted['Accuracy_test'] <- confusionMatrix(data = y_hat4, reference = test4$Credit_Application_Result)$overall["Accuracy"]  

set.seed(1)
model4w <- train(worthy~., 
                 data=train4[!colnames(train) %in% c('Credit_Application_Result', 'nonworthy')], 
                 method='bstTree',
                 trControl = control)
y_hat4w <- predict(model4w, test4)
boosted['Accuracy_worthy'] <- confusionMatrix(data = y_hat4w, reference = test4$worthy)$overall["Accuracy"]

set.seed(1)
model4n <- train(nonworthy~., 
                 data=train4[!colnames(train) %in% c('Credit_Application_Result', 'worthy')], 
                 method='bstTree',
                 trControl = control)
y_hat4n <- predict(model4n, test4)
boosted['Accuracy_nonworthy'] <- confusionMatrix(data = y_hat4n, reference = test4$nonworthy)$overall["Accuracy"]

vi4 <- varImp(model4, scale = F)   #pvalue
plot4 <- ggplot(vi4, top = dim(vi4$importance)[1]) + 
  ggtitle('Variable Importance - Boosted Tree')

#Print confusion matrices
print(confusionMatrix(data = y_hat1, reference = test$Credit_Application_Result))
print(confusionMatrix(data = y_hat2, reference = test$Credit_Application_Result))
print(confusionMatrix(data = y_hat3, reference = test$Credit_Application_Result))
print(confusionMatrix(data = y_hat4, reference = test4$Credit_Application_Result))
```
### Overall accuracy
Among the 4 models, Logistic regression has the highest overall accuracy agains the Validation set (0.83). The bias can be seen in Logistic regression, Random forest, and Boosted tree, since accuracy against train set is lower than validation test for those models. 
```{r acc train test}
print(data.frame(lr, tree, forest, boosted)[c('Accuracy_train','Accuracy_test'),])
```
### Plot variable importance
Most important predictors:
- For Logistic regression: Account_Balance, Payment_Status_of_Previous_Credit, Credit_Amoun, Value_Savings_Stocks, Purpose, Instalment_per_cent.
- For Decision tree: Payment_Status_of_Previous_Credit, Credit_Amount, Value_Savings_Stocks, Account_Balance, Duration_of_Credit_Month.
- For Random forest: Credit_Amount, Age_years, Duration_of_Credit_Month.
- For Boosted tree: Account_Balance, Value_Savings_Stocks, Paymen_Status_of_Previous_Credit, and Duration_of_Credit_Month are the 4 most important variables. The difference is not remarkable among the fields.
```{r plot1}
plot1
plot2
plot3
plot4
```

## Step 4: Writeup
Accuracy table
```{r acc}
print(data.frame(lr, tree, forest, boosted))
```

Plot ROCs 
```{r plot2}
par(pty = "s")
roc(as.numeric(test$Credit_Application_Result), 
    as.numeric(y_hat1), plot=TRUE,
    legacy.axes=TRUE, percent=TRUE, 
    xlab="False Positive Percentage", 
    ylab="True Postive Percentage", 
    print.auc=TRUE,
    print.auc.x=45,
    ) 
legend("bottomright", legend=c("Logisitic Regression"))

plot.roc(as.numeric(test$Credit_Application_Result),
         as.numeric(y_hat2),
         legacy.axes=TRUE, percent=TRUE,
         xlab="False Positive Percentage",
         ylab="True Postive Percentage",
         print.auc=TRUE,
         print.auc.x=45,
)
legend("bottomright", legend=c("Decision Tree"))

plot.roc(as.numeric(test$Credit_Application_Result),
         as.numeric(y_hat3),
         legacy.axes=TRUE, percent=TRUE,
         xlab="False Positive Percentage",
         ylab="True Postive Percentage",
         print.auc=TRUE,
         print.auc.x=45,
)
legend("bottomright", legend=c("Random Forest"))

plot.roc(as.numeric(test$Credit_Application_Result),
         as.numeric(y_hat4),
         legacy.axes=TRUE, percent=TRUE,
         xlab="False Positive Percentage",
         ylab="True Postive Percentage",
         print.auc=TRUE,
         print.auc.x=45,
)
legend("bottomright", legend=c("Boosted Tree"))
```
Logistic regresson model has the highest overall accuracy against the Validation set, as well as the highest accuracy within segments. It is also has the most optimal ROC. Therefore, I choose to use logistic regression model.
Applying the chosen model, the number of individuals are creditworthy is 413.
```{r predict}
yhat_final <- predict(model1, validation)
print(sum(yhat_final=='Creditworthy'))
```